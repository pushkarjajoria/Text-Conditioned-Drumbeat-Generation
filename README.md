
<p align="center">
  <img src="images/University_Of_Galway_Logo__Positive_Portrait.jpg" height="90"/>
  <img src="images/lsv-logo_no-background_page.jpg" height="90"/>
  <img src="images/uds-eule_page-0001.jpg" height="90"/>
</p>

# BeatBrewer: Text-Conditioned Drumbeat Generation

> üé∂ Published at [SMC 2025 ‚Äî Sound and Music Computing Conference](https://smc25.iem.at/), Graz

This repository provides code for generating symbolic drumbeats from natural language descriptions (text prompts) using **Latent Diffusion Models (LDMs)**. This was developed as part of the research presented in our SMC 2025 paper and introduces a multi-stage training pipeline combining multimodal pretraining, autoencoding, and diffusion modeling.

---

## üß† Project Overview

<p align="center">
  <img src="images/Generation_Flow_Horizontal.png" width="100%"/>
</p>

We present a three-stage pipeline to synthesize drumbeats conditioned on text:

1. **Multimodal Pretraining**  
   Pretrain a text encoder and MIDI encoder into a **joint embedding space** using contrastive loss (inspired by CLIP).  
   üìÅ See: `text_supervised_pretraining/`

   <p align="center">
     <img src="images/Text_Supervised_Pretraining.png" width="70%"/>
   </p>

2. **Autoencoder Training**  
   Use a novel **MultiResolutionLSTM** to learn a latent representation of MIDI pianorolls. The model is trained to reconstruct the original sequence from the latent.  
   üìÅ See: `Midi_Encoder/`

   <p align="center">
     <img src="images/Autoencoder.png" width="70%"/>
   </p>

3. **Latent Diffusion Modeling**  
   A DDPM is trained to denoise latent vectors over 1000 timesteps, conditioned on text embeddings from the pretrained encoder.  
   üìÑ Script: `DDPM/main_latent_space.py`  
   üîÑ Inference: `DDPM/demo.py`

---

## üì¶ Installation & Inference

> ‚ö†Ô∏è **Note:** Model weights are _not_ included in this repository at the moment due to licensing checks with the data provider.

### üîß Setup

1. Create a Conda environment and install dependencies:

```bash
# Create and activate env (Python 3.10+ recommended)
conda create -n beatbrewer python=3.10
conda activate beatbrewer

# Install dependencies
pip install pyyaml tqdm torch numpy matplotlib scikit-learn transformers
pip install git+https://github.com/jmmcd/python-midi/
````

2. Clone this repo:

```bash
git clone git@github.com:pushkarjajoria/BeatBrewer.git
cd BeatBrewer
```

3. Set Python path:

```bash
export PYTHONPATH=$(pwd):$PYTHONPATH
```

4. Place the pretrained weights in a folder called `demo_models/` in the root directory.

### ‚ñ∂Ô∏è Run Inference

```bash
python DDPM/demo.py
```

* Generated MIDI files will appear in `demo_midi_files/`.
* Each drumbeat is conditioned on a prompt derived from the Groove Monkee dataset filenames.

---

## üìä Evaluation

<p align="center">
  <img src="images/Survey Results.png" width="70%"/>
</p>

We evaluated the generated drumbeats using:

* **Binary pianoroll & latent distance** to assess novelty
* **Listening tests** for quality and prompt relevance

---

## üìö Citation

If you use this code or refer to the paper, please cite:

```bibtex
@inproceedings{jajoria_2025_15838008,
  author       = {Jajoria, Pushkar and Klakow, Dietrich and McDermott, James},
  title        = {Text Conditioned Symbolic Drumbeat Generation using Latent Diffusion Models},
  booktitle    = {Proc. 22nd Sound and Music Computing Conference (SMC2025)},
  year         = {2025},
  pages        = {311--318},
  publisher    = {IEM, University of Music and Performing Arts Graz},
  month        = jul,
  venue        = {Graz, Austria},
  doi          = {10.5281/zenodo.15838008},
  url          = {https://doi.org/10.5281/zenodo.15838008},
}
```

---
